## 2.14 Functions and triggers (supplementary material)

  * [Timestamping changes in the database using triggers](#c_2.14.1)
  * [Automation of the GPS data association with animal](#c_2.14.2)
  * [Consistency checks on the deployments information](#c_2.14.3)
  * [Synchronization of *gps\_sensors\_animals* and *gps\_data\_animals*](#c_2.14.4)
  * [Automation of the creation of points from GPS coordinates](#c_2.14.5)
  * [UTM zone of a given point in geographic coordinates](#c_2.14.6)
  * [Automation of the intersection of locations with environmental layers](#c_2.14.7)
  * [Detect records that would imply impossible or improbable movements](#c_2.14.8)

**NOTE:** *This section (supplementary code) is meant to provide advanced examples of how database tools can be used to improve the management of tracking data. The code itself is introduced to illustrate the goals and functionalities but the technical details are not explained because they require an advanced knowledge of database programming. The idea is that this supplementary code can be used as it is or as a study example for they who want to explore and learn advances features offered by spatial database.*

### <a name="c_2.14.1"></a>Timestamping changes in the database using triggers

It can often be useful to know not only when a record is created but also the last time that a record has been modified and who modified it. This is important to keep track of what happens in the database. This can be achieved using two powerful tools: functions and triggers.

A **[function](http://www.postgresql.org/docs/devel/static/xfunc-sql.html)** is a program code that is implemented inside the database using SQL or a set of other languages (e.g. SQL, PSQL, Python, C). Functions allow you to create complex processes and algorithms when SQL queries alone cannot do the job. Once created, a function becomes part of the database library and can be called inside SQL queries. In the framework of these lessons, you do not need to create your own functions, but you must be aware of the possibility offered by these tools and be able to understand and use existing functions that advanced users can adapt according to their specific needs.

A **[trigger](http://www.postgresql.org/docs/devel/static/triggers.html)** is a specification that the database should automatically execute a particular function whenever a certain type of operation is performed on a particular table in the database. The trigger fires a specific function to perform some actions BEFORE or AFTER records are DELETED, UPDATED, or INSERTED in a table. The trigger function must be defined before the trigger itself can be created. The trigger function must be declared as a function taking no arguments and returning type trigger. For example, when you insert a new record in a table, you can modify the values of the attributes before they are uploaded or you can update another table that should be affected by this new upload. It is important to stress that triggers are very powerful tools for automating the data flow. The drawback is that they will slow down the data import process. This note is also valid for indexes, which speed up queries but imply some additional computation during the import stage. In the case of frequent uploads (or modification) of very large data sets at once, the use of the proposed triggers could significantly decrease performance. In these cases, you can more quickly process the data in a later stage after they are imported into the database and therefore available to users. The best approach must be identified according to the specific goals, constraints, and characteristics of your application. In this guide, we use as reference the management of data coming from a set of sensors deployed on animals, transmitting data in near real time, where the import step will include just few thousand locations at a time. 

It might be convenient to store all functions and ancillary tools in a defined schema:

```sql
CREATE SCHEMA tools
  AUTHORIZATION postgres;
  GRANT USAGE ON SCHEMA tools TO basic_user;
```

```sql
COMMENT ON SCHEMA tools 
IS 'Schema that hosts all the functions and ancillary tools used for the database.';
```

```sql
ALTER DEFAULT PRIVILEGES 
  IN SCHEMA tools 
  GRANT SELECT ON TABLES 
  TO basic_user;
```

Here a simple example of an SQL function that makes the sum of two input integers:

```sql
CREATE FUNCTION tools.test_add(integer, integer) 
  RETURNS integer AS 
'SELECT $1 + $2;'
LANGUAGE SQL
IMMUTABLE
RETURNS NULL ON NULL INPUT;
```

The variables *$1* and *$2* are the first and second input parameters. You can test it with

```sql
SELECT tools.test_add(2,7);
```

As a first simple example of a trigger, you add a field to the table *gps\_data\_animals* where you register the timestamp of the last modification (update) of each record in order to keep track of the changes in the table. This field can have *now()*as default when data is inserted the first time:

```sql
ALTER TABLE main.gps_data_animals 
  ADD COLUMN update_timestamp timestamp with time zone DEFAULT now();
```

Once you have created the field, you need a function called by a trigger to set this field to the timestamp of the change time whenever a record is updated. The SQL to generate the function is:

```sql
CREATE OR REPLACE FUNCTION tools.timestamp_last_update()
RETURNS trigger AS
$BODY$BEGIN
IF NEW IS DISTINCT FROM OLD THEN
  NEW.update_timestamp = now();
END IF;
RETURN NEW;
END;$BODY$
LANGUAGE plpgsql VOLATILE
COST 100;
```

```sql
COMMENT ON FUNCTION tools.timestamp_last_update() 
IS 'When a record is updated, the update_timestamp is set to the current time.';
```

Here is the code for the trigger that calls the function:

```sql
CREATE TRIGGER update_timestamp
  BEFORE UPDATE
  ON main.gps_data_animals
  FOR EACH ROW
  EXECUTE PROCEDURE tools.timestamp_last_update();
```

You have to initialize the existing records in the table, as the trigger/function was not yet created when data were uploaded:

```sql
UPDATE main.gps_data_animals 
  SET update_timestamp = now();
```

Another interesting application of triggers is the automation of the *acquisition\_time* computation when a new record is inserted into the *gps\_data* table:

```sql
CREATE OR REPLACE FUNCTION tools.acquisition_time_update()
RETURNS trigger AS
$BODY$BEGIN
  NEW.acquisition_time = ((NEW.utc_date + NEW.utc_time) at time zone 'UTC');
  RETURN NEW;
END;$BODY$
LANGUAGE plpgsql VOLATILE
COST 100;
```

```sql
COMMENT ON FUNCTION tools.acquisition_time_update() 
IS 'When a record is inserted, the acquisition_time is composed from utc_date and utc_time.';
```

```sql
CREATE TRIGGER update_acquisition_time
  BEFORE INSERT
  ON main.gps_data
  FOR EACH ROW
  EXECUTE PROCEDURE tools.acquisition_time_update();
```

### <a name="c_2.14.2"></a>Automation of the GPS data association with animals
In the case of a large number of sensors and animals, the association of locations to animals is hard to manage manually, and usually requires some dedicated, and possibly automated, tools. Moreover, the process of associating GPS positions and animals must be able to manage dynamic changes in the information about sensor deployment. For example, hours or even days can pass before the death of an animal tagged with a GPS sensor is discovered. In the while, the GPS positions acquired in near real time are associated with the animal. This is an error, as the positions recorded between the death and its detection by researchers are not valid and must be 'disassociated' from the animal. A tool to automatically and dynamically update the association between animals and GPS location based on the information stored in the table on sensors deployment would also efficiently manages the re-deployment of a GPS sensor recovered from an animal (because of e.g. end of battery or death of the animal) to another animal, and the deployment of a new GPS sensor on an animal previously monitored with another GPS sensor.

With triggers and functions, you can automatize the upload from *gps\_data* to *gps\_data\_animals* of records that are associated with animals (a sensor deployed on an animal). First, you have to create the function that will be called by the trigger:

```sql
CREATE OR REPLACE FUNCTION tools.gps_data2gps_data_animals()
RETURNS trigger AS
$BODY$ begin
INSERT INTO main.gps_data_animals (
  animals_id, gps_sensors_id, acquisition_time, longitude, latitude)
SELECT 
  gps_sensors_animals.animals_id, gps_sensors_animals.gps_sensors_id, NEW.acquisition_time, NEW.longitude, NEW.latitude
FROM 
  main.gps_sensors_animals, main.gps_sensors
WHERE 
  NEW.gps_sensors_code = gps_sensors.gps_sensors_code AND 
  gps_sensors.gps_sensors_id = gps_sensors_animals.gps_sensors_id AND
  (
    (NEW.acquisition_time >= gps_sensors_animals.start_time AND 
     NEW.acquisition_time <= gps_sensors_animals.end_time)
    OR 
    (NEW.acquisition_time >= gps_sensors_animals.start_time AND 
     gps_sensors_animals.end_time IS NULL)
  );
RETURN NULL;
END
$BODY$
LANGUAGE plpgsql VOLATILE
COST 100;
```

```sql
COMMENT ON FUNCTION tools.gps_data2gps_data_animals() 
IS 'Automatic upload data from gps_data to gps_data_animals.';
```

Then, you create a trigger that calls the function whenever a new record is uploaded into \*gps\_data\*:

```sql
CREATE TRIGGER trigger_gps_data_upload
  AFTER INSERT
  ON main.gps_data
  FOR EACH ROW
  EXECUTE PROCEDURE tools.gps_data2gps_data_animals();
```

```sql
COMMENT ON TRIGGER trigger_gps_data_upload ON main.gps_data
IS 'Upload data from gps_data to gps_data_animals whenever a new record is inserted.';
```

You can test this function by removing and then adding again one of GPS sensor data sets. Now data are automatically processed and imported into the table *gps\_data\_animals* including the correct association with the animal wearing the sensor.


### <a name="c_2.14.3"></a>Consistency checks on the deployments information

The management of the association between animals and GPS sensors can be further improved using additional, more sophisticated tools. A first example is the implementation of consistency checks on the *gps\_sensors\_animals* table. You already created a check to ensure that the *start\_date* &lt; *end\_date*, but this is not enough to prevent illogical associations between animals and sensors. The two most evident constraints are that the same sensor cannot be worn by two animals at the same time, and that no more than one GPS sensor can be deployed on the same animal at the same time (this assumption can be questionable in case of other sensors, but in general can be considered valid for GPS). To avoid any impossible overlaps in animal/sensor deployments, you have to create a trigger on both insertion and updates of records in *gps\_animals\_sensors* that verifies the correctness of the new values (i.e. the new deployment interval is not in conflict with other existing deployments). **[NEW](http://www.postgresql.org/docs/devel/static/plpgsql-trigger.html)** in a BEFORE INSERT/UPDATE trigger refers to the values that are going to be inserted. In an UPDATE/DELETE trigger, **[OLD](http://www.postgresql.org/docs/devel/static/plpgsql-trigger.html)** refers to the value that is going to be modified. In case of invalid values, the insert/modify statement is aborted and an error message is raised by the database. Here is an example of code for this function:

```sql
CREATE OR REPLACE FUNCTION tools.gps_sensors_animals_consistency_check()
RETURNS trigger AS
$BODY$
DECLARE
  deletex integer;
BEGIN

SELECT 
  gps_sensors_animals_id 
INTO 
  deletex 
FROM 
  main.gps_sensors_animals b
WHERE
  (NEW.animals_id = b.animals_id OR NEW.gps_sensors_id = b.gps_sensors_id)
  AND
  (
  (NEW.start_time > b.start_time AND NEW.start_time < b.end_time)
  OR
  (NEW.start_time > b.start_time AND b.end_time IS NULL)
  OR
  (NEW.end_time > b.start_time AND NEW.end_time < b.end_time)
  OR
  (NEW.start_time < b.start_time AND NEW.end_time > b.end_time)
  OR
  (NEW.start_time < b.start_time AND NEW.end_time IS NULL )
  OR
  (NEW.end_time > b.start_time AND b.end_time IS NULL)
);

IF deletex IS not NULL THEN
  IF TG_OP = 'INSERT' THEN
    RAISE EXCEPTION 'This row is not inserted: Animal-sensor association not valid: (the same animal would wear two different GPS sensors at the same time or the same GPS sensor would be deployed on two animals at the same time).';
    RETURN NULL;
  END IF;
  IF TG_OP = 'UPDATE' THEN
    IF deletex != OLD.gps_sensors_animals_id THEN
      RAISE EXCEPTION 'This row is not updated: Animal-sensor association not valid (the same animal would wear two different GPS sensors at the same time or the same GPS sensor would be deployed on two animals at the same time).';
      RETURN NULL;
    END IF;
  END IF;
END IF;

RETURN NEW;
END;
$BODY$
LANGUAGE plpgsql VOLATILE
COST 100;
```

```sql
COMMENT ON FUNCTION tools.gps_sensors_animals_consistency_check() 
IS 'Check if a modified or insert row in gps_sensors_animals is valid (no impossible time range overlaps of deployments).';
```

Here is an example of the trigger to call the function:

```sql
CREATE TRIGGER gps_sensors_animals_changes_consistency
  BEFORE INSERT OR UPDATE
  ON main. gps_sensors_animals
  FOR EACH ROW
  EXECUTE PROCEDURE tools.gps_sensors_animals_consistency_check();
```

You can test this process by trying to insert a deployment of a GPS sensor in the `gps_sensors_animals` table in a time interval that overlaps the association of the same sensor on another animal:

```sql
INSERT INTO main.gps_sensors_animals
  (animals_id, gps_sensors_id, start_time, end_time, notes)
VALUES
  (2,2,'2004-10-23 20:00:53 +0','2005-11-28 13:00:00 +0','Ovelapping sensor');
```

You should receive an error message like:

```
> **\*\*\**** Error ***\*\**\**** ERROR: This row is not inserted:
Animal-sensor association not valid: (the same animal would wear two
different GPS sensors at the same time or the same GPS sensor would be
deployed on two animals at the same time). SQL state: P0001
```

### <a name="c_2.14.4"></a>Synchronization of *gps\_sensors\_animals* and *gps\_data\_animals*

In an operational environment where data are managed in (near) real time, it happens that the information about the association between animals and sensors changes over time. A typical example is the death of an animal: this event is usually discovered with a delay of some days. In the meantime, GPS positions are received and associated with the animals in the *gps\_data\_animals* table. When the new information on the deployment time range is registered in*gps\_sensors\_animals*, the table *gps\_data\_animals* must be changed accordingly. It is highly desirable that any change in the table *gps\_sensors\_animals* is automatically reflected in *gps\_data\_animals*. It is possible to use triggers to keep the two tables automatically synchronized in real time. Here below you have an example of a trigger function to implement this procedure. The code is fairly complex because it manages the three possible operations: delete, insert, and modification of the *gps\_sensors\_animals* table. For each case, it checks whether GPS positions previously associated with an animal are no longer valid (and if so, deletes them from the table *gps\_data\_animals*) and whether GPS positions previously not associated with the animal should now be linked (and if so, adds them to the table*gps\_data\_animals*).

```sql
CREATE OR REPLACE FUNCTION tools.gps_sensors_animals2gps_data_animals()
RETURNS trigger AS
$BODY$ begin

IF TG_OP = 'DELETE' THEN

  DELETE FROM 
    main.gps_data_animals 
  WHERE 
    animals_id = OLD.animals_id AND
    gps_sensors_id = OLD.gps_sensors_id AND
    acquisition_time >= OLD.start_time AND
    (acquisition_time <= OLD.end_time OR OLD.end_time IS NULL);
  RETURN NULL;

END IF;

IF TG_OP = 'INSERT' THEN

  INSERT INTO 
    main.gps_data_animals (gps_sensors_id, animals_id, acquisition_time, longitude, latitude)
  SELECT 
    NEW.gps_sensors_id, NEW.animals_id, gps_data.acquisition_time, gps_data.longitude, gps_data.latitude
  FROM 
    main.gps_data, main.gps_sensors
  WHERE 
    NEW.gps_sensors_id = gps_sensors.gps_sensors_id AND
    gps_data.gps_sensors_code = gps_sensors.gps_sensors_code AND
    gps_data.acquisition_time >= NEW.start_time AND
    (gps_data.acquisition_time <= NEW.end_time OR NEW.end_time IS NULL);
  RETURN NULL;

END IF;

IF TG_OP = 'UPDATE' THEN

  DELETE FROM 
    main.gps_data_animals 
  WHERE
    gps_data_animals_id IN (
      SELECT 
        d.gps_data_animals_id 
      FROM
        (SELECT 
          gps_data_animals_id, gps_sensors_id, animals_id, acquisition_time 
        FROM 
          main.gps_data_animals
        WHERE 
          gps_sensors_id = OLD.gps_sensors_id AND
          animals_id = OLD.animals_id AND
          acquisition_time >= OLD.start_time AND
          (acquisition_time <= OLD.end_time OR OLD.end_time IS NULL)
        ) d
      LEFT OUTER JOIN
        (SELECT 
          gps_data_animals_id, gps_sensors_id, animals_id, acquisition_time 
        FROM 
          main.gps_data_animals
        WHERE 
          gps_sensors_id = NEW.gps_sensors_id AND
          animals_id = NEW.animals_id AND
          acquisition_time >= NEW.start_time AND
          (acquisition_time <= NEW.end_time OR NEW.end_time IS NULL) 
        ) e
      ON 
        (d.gps_data_animals_id = e.gps_data_animals_id)
      WHERE e.gps_data_animals_id IS NULL);

  INSERT INTO 
    main.gps_data_animals (gps_sensors_id, animals_id, acquisition_time, longitude, latitude) 
  SELECT 
    u.gps_sensors_id, u.animals_id, u.acquisition_time, u.longitude, u.latitude 
  FROM
    (SELECT 
      NEW.gps_sensors_id AS gps_sensors_id, NEW.animals_id AS animals_id, gps_data.acquisition_time AS acquisition_time, gps_data.longitude AS longitude, gps_data.latitude AS latitude
    FROM 
      main.gps_data, main.gps_sensors
    WHERE 
      NEW.gps_sensors_id = gps_sensors.gps_sensors_id AND 
      gps_data.gps_sensors_code = gps_sensors.gps_sensors_code AND
      gps_data.acquisition_time >= NEW.start_time AND
      (acquisition_time <= NEW.end_time OR NEW.end_time IS NULL)
    ) u
  LEFT OUTER JOIN
    (SELECT 
      gps_data_animals_id, gps_sensors_id, animals_id, acquisition_time 
    FROM 
      main.gps_data_animals
    WHERE 
      gps_sensors_id = OLD.gps_sensors_id AND
      animals_id = OLD.animals_id AND
      acquisition_time >= OLD.start_time AND
      (acquisition_time <= OLD.end_time OR OLD.end_time IS NULL)
    ) w
  ON 
    (u.gps_sensors_id = w.gps_sensors_id AND 
    u.animals_id = w.animals_id AND 
    u.acquisition_time = w.acquisition_time )
  WHERE 
    w.gps_data_animals_id IS NULL;
  RETURN NULL;

END IF;

END;$BODY$
LANGUAGE plpgsql VOLATILE
COST 100;
```

```sql
COMMENT ON FUNCTION tools.gps_sensors_animals2gps_data_animals() 
IS 'When a record in gps_sensors_animals is deleted OR updated OR inserted, this function synchronizes this information with gps_data_animals.';
```

Here is the code of the trigger to call the function:

```sql
CREATE TRIGGER synchronize_gps_data_animals
  AFTER INSERT OR UPDATE OR DELETE
  ON main.gps_sensors_animals
  FOR EACH ROW
  EXECUTE PROCEDURE tools.gps_sensors_animals2gps_data_animals();
```


### <a name="c_2.14.5"></a>Automation of the creation of points from GPS coordinates

Working with massive data sets (i.e. many sensors at the same time) in near real time requires that routinely operations are done automatically to save time and to avoid errors of manual processing. Here you create a new function to update the geometry field as soon as a new record is uploaded. 

You can automate the population of the geometry column so that whenever a new GPS position is updated in the table *main.gps\_data\_animals*, the spatial geometry is also created. To do so, you need a trigger and its related function. Here is the SQL code to generate the function:

```sql
CREATE OR REPLACE FUNCTION tools.new_gps_data_animals()
RETURNS trigger AS
$BODY$
DECLARE 
thegeom geometry;
BEGIN

IF NEW.longitude IS NOT NULL AND NEW.latitude IS NOT NULL THEN
  thegeom = ST_SetSRID(ST_MakePoint(NEW.longitude, NEW.latitude),4326);
  NEW.geom = thegeom;
END IF;

RETURN NEW;
END;$BODY$
LANGUAGE plpgsql VOLATILE
COST 100;
```

```sql
COMMENT ON FUNCTION tools.new_gps_data_animals() 
IS 'When called by a trigger (insert_gps_locations) this function populates the field geom using the values from longitude and latitude fields.';
```

And here is the SQL code to generate the trigger:

```sql
CREATE TRIGGER insert_gps_location
  BEFORE INSERT
  ON main.gps_data_animals
  FOR EACH ROW
  EXECUTE PROCEDURE tools.new_gps_data_animals();
```

You can see the result by deleting all the records from the *main.gps\_data\_animals* table, e.g. for animal 2, and reloading them. As you have set an automatic procedure to synchronize *main.gps\_data\_animals* table with the information contained in the table *main.gps\_sensors\_animals*, you can drop the animal 2 record from *main.gps\_sensors\_animals* and this will affect *main.gps\_data\_animals* in a cascade effect (note that it will not affect the original data in *main.gps\_data*):

```sql
DELETE FROM 
  main.gps_sensors_animals 
WHERE 
  animals_id = 2;
```

There are now no rows for animal 2 in the table *main.gps\_data\_animals*. You can verify this by retrieving the number of locations per animal:

```sql
SELECT 
  animals_id, count(animals_id) 
FROM 
  main.gps_data_animals
GROUP BY 
  animals_id
ORDER BY 
  animals_id;
```

Note that animal 2 is not in the list. Now you reload the record in the *main.gps\_sensors\_animals*:

```sql
INSERT INTO main.gps_sensors_animals 
  (animals_id, gps_sensors_id, start_time, end_time, notes) 
VALUES 
  (2,1,'2005-03-20 16:03:14 +0','2006-05-27 17:00:00 +0','End of battery life. Sensor not recovered.');
```

You can see that records have been re-added to *main.gps\_data\_animals* by reloading the original data stored in *main.gps\_data*, with the geometry field correctly and automatically populated (when longitude and latitude are not null):

```sql
SELECT 
  animals_id, count(animals_id) AS num_records, count(geom) AS num_records_valid 
FROM 
  main.gps_data_animals
GROUP BY 
  animals_id
ORDER BY 
  animals_id;
```

You can now play around with your spatial data set. For example, when you have a number of locations per animal, you can find the centroid of the area covered by the locations:

```sql
SELECT 
  animals_id, 
  ST_AsEWKT(
    ST_Centroid(
     ST_Collect(geom))) AS centroid 
FROM 
  main.gps_data_animals 
WHERE 
  geom IS NOT NULL 
GROUP BY 
  animals_id 
ORDER BY 
  animals_id;
```

In this case you used the SQL command **[ST\_Collect](http://postgis.refractions.net/docs/ST_Collect.html)**. This function returns a GEOMETRYCOLLECTION or a MULTI object from a set of geometries. The collect function is an 'aggregate' function in the terminology of PostgreSQL. That means that it operates on rows of data, in the same way the sum() and mean() functions do. *ST\_Collect* and **[ST\_Union](http://postgis.refractions.net/docs/ST_Union.html)** are often interchangeable. *ST\_Collect* is in general orders of magnitude faster than *ST\_Union* because it does not try to dissolve boundaries. It merely rolls up single geometries into MULTI and MULTI or mixed geometry types into Geometry Collections. The contrary of *ST\_Collect* is **[ST\_Dump](http://postgis.refractions.net/docs/ST_Dump.html)**, which is a set-returning function.

### <a name="c_2.14.6"></a>UTM zone of a given point in geographic coordinates

Here you create a simple function to automatically find the UTM zone at defined coordinates:

```sql
CREATE OR REPLACE FUNCTION tools.srid_utm(longitude double precision, latitude double precision)
RETURNS integer AS
$BODY$
DECLARE
  srid integer;
  lon float;
  lat float;
BEGIN
  lat := latitude;
  lon := longitude;

IF ((lon > 360 or lon < -360) or (lat > 90 or lat < -90)) THEN 
  RAISE EXCEPTION 'Longitude and latitude is not in a valid format (-360 to 360; -90 to 90)';
ELSEIF (longitude < -180)THEN 
  lon := 360 + lon;
ELSEIF (longitude > 180)THEN 
  lon := 180 - lon;
END IF;

IF latitude >= 0 THEN 
  srid := 32600 + floor((lon+186)/6); 
ELSE
  srid := 32700 + floor((lon+186)/6); 
END IF;

RETURN srid;
END;
$BODY$
LANGUAGE plpgsql VOLATILE STRICT
COST 100;
```

```sql
COMMENT ON FUNCTION tools.srid_utm(double precision, double precision) 
IS 'Function that returns the SRID code of the UTM zone where a point (in geographic coordinates) is located. For polygons or line, it can be used giving ST_x(ST_Centroid(the_geom)) and ST_y(ST_Centroid(the_geom)) as parameters. This function is typically used be used with ST_Transform to project elements with no prior knowledge of their position.';
```

Here an example to see the SRID of the UTM zone of the point at
coordinates (11.001,46.001):

```sql
SELECT TOOLS.SRID_UTM(11.001,46.001) AS UTM_zone;
```

The result 32632 corresponds to UTM 32 N WGS84.

You can use this function to project points when you do not know the UTM zone. You can test this functionality with the following code:

```sql
SELECT
  ST_AsEWKT(
    ST_Transform(
      ST_SetSRID(ST_MakePoint(31.001,16.001), 4326),
      TOOLS.SRID_UTM(31.001,16.001))
  ) AS projected_point;
```

If you want to allow the user `basic_user` to project spatial data, you have to grant permission on the table `spatial_ref_sys`:

```sql
GRANT SELECT ON TABLE spatial_ref_sys TO basic_user;
```

### <a name="c_2.14.7"></a>Automation of the intersection of locations with environmental layers

The next step is to implement the computation of these parameters inside the automated process of associating GPS positions with animals (from *gps\_data* to *gps\_data\_animals*). To achieve this goal, you have to modify the trigger function *tools.new\_gps\_data\_animals*. In fact, the function *tools.new\_gps\_data\_animals* is activated whenever a new location is inserted into *gps\_data\_animals* (from *gps\_data*). It adds new information (i.e. fills additional fields) to the incoming record (e.g. creates the geometry object from latitude and longitude values) before it is uploaded into the *gps\_data\_animals* table (in the code, "NEW." Is used to reference the new record not yet inserted). The SQL code that does this is below. The drawback of this function is that it will slow down the import of a large set of positions at once (e.g. millions or more), but it has no practical impact when you manage a continuous data flow from sensors, even for a large number of sensors deployed at the same time.

```sql
CREATE OR REPLACE FUNCTION tools.new_gps_data_animals()
RETURNS trigger AS
$BODY$
DECLARE 
  thegeom geometry;
BEGIN

IF NEW.longitude IS NOT NULL AND NEW.latitude IS NOT NULL THEN
  thegeom = ST_SetSRID(ST_MakePoint(NEW.longitude, NEW.latitude), 4326);
  NEW.geom =thegeom;
  NEW.pro_com = 
    (SELECT pro_com::integer 
    FROM env_data.adm_boundaries 
    WHERE ST_Intersects(geom,thegeom)); 
  NEW.station_id = 
    (SELECT station_id::integer 
    FROM env_data.meteo_stations 
    ORDER BY ST_DistanceSpheroid(thegeom, geom, 'SPHEROID["WGS 84",6378137,298.257223563]') 
    LIMIT 1);
  NEW.roads_dist = 
    (SELECT ST_Distance(thegeom::geography, geom::geography)::integer 
    FROM env_data.roads 
    ORDER BY ST_distance(thegeom::geography, geom::geography) 
    LIMIT 1);
END IF;

RETURN NEW;
END;$BODY$
LANGUAGE plpgsql VOLATILE
COST 100;
ALTER FUNCTION tools.new_gps_data_animals()
OWNER TO postgres;
```

```sql
COMMENT ON FUNCTION tools.new_gps_data_animals() 
IS 'When called by the trigger insert_gps_positions (raised whenever a new position is uploaded into gps_data_animals) this function gets the longitude and latitude values and sets the geometry field accordingly, computing a set of derived environmental information calculated intersecting or relating the position with the environmental ancillary layers.';
```

As the trigger function is run during location import, the function only works on the locations that are imported after it was created, and not on previously imported locations. To see the effects, you have to add new positions or delete and reload the GPS position stored in *gps\_data\_animals*. You can do this by saving the records in *gps\_sensors\_animals* in an external .csv file, and then deleting the records from the table (which also deletes the records in *gps\_data\_animals* in a cascading effect). When you reload them, the new function will be activated by the trigger that was just defined, and the new attributes will be calculated. You can perform these steps with the following commands. First, check how many records you have per animal:

```sql
SELECT animals_id, count(animals_id) 
FROM main.gps_data_animals 
GROUP BY animals_id;
```

Then, copy of the table *main.gps\_sensors\_animals* into an external file.

```sql
COPY 
  (SELECT animals_id, gps_sensors_id, start_time, end_time, notes 
FROM main.gps_sensors_animals)
TO 
  'c:/tracking_db/test/gps_sensors_animals.csv' 
  WITH (FORMAT csv, DELIMITER ';');
```

You then delete all the records in *main.gps\_sensors\_animals*, which will delete (in a cascade) all the records
in *main.gps\_data\_animals*.

```sql
DELETE FROM main.gps_sensors_animals;
```

You can verify that there are now no records are in *main.gps\_data\_animals* (the query should return 0 rows).

```sql
SELECT * FROM main.gps_data_animals;
```

The final step is to reload the .csv file into *main.gps\_sensors\_animals*. This will launch the trigger functions that recreate all the records in *main.gps\_data\_animals*, in which the fields related to environmental attributes are also automatically updated. Note that, due to the different triggers which imply massive computations, the query can take several minutes to execute (you can skip this step and speed up the process by simply calculating the environmental attributes with an update query).

```sql
COPY main.gps_sensors_animals 
  (animals_id, gps_sensors_id, start_time, end_time, notes) 
FROM 
  'c:/tracking_db/test/gps_sensors_animals.csv' 
  WITH (FORMAT csv, DELIMITER ';');
```

You can verify that all the fields were updated:

```sql
SELECT 
  gps_data_animals_id AS id, acquisition_time, pro_com, station_id AS meteo, roads_dist AS dist
FROM 
  main.gps_data_animals 
WHERE 
  geom IS NOT NULL
LIMIT 5;
```

You can also check that all the records of every animal are in *main.gps\_data\_animals*:

```sql
SELECT animals_id, count(*) FROM main.gps_data_animals GROUP BY animals_id;
```

As you can see, the whole process can take a few minutes, as you are calculating the environmental attributes for the whole data set at once. As discussed in the previous chapters, the use of triggers and indexes to automatize data flow and speed up analyses might imply processing times that are not sustainable when large data sets are imported at once. In this case, it might be preferable to update environmental attributes and calculate indexes in a later stage to speed up the import process. In this book, we assume that in the operational environment where the database is developed, the data flow is continuous, with large but still limited data sets imported at intervals. You can compare this processing time with what is generally required to achieve the same result in a classic GIS environment based on flat files (e.g. shapefiles, .tif). Do not forget to consider that you can use these minutes for a coffee break while the database does the job for you, instead of clicking here and there in your favorite GIS application! 

### <a name="c_2.14.8"></a>Detect records that would imply impossible or improbable movements

To detect records with incorrect coordinates that cannot be identified using clear boundaries, such as the study area or land cover type, a more sophisticated outlier filtering procedure must be applied. To do so, some kind of assumption about the animals' movement model has to be made, for example a speed limit. It is important to remember that animal movements can be modelled in different ways at different temporal scales: an average speed that is impossible over a period of four hours could be perfectly feasible for movements in a shorter time (e.g. five minutes). Which algorithm to apply depends largely on the species and the environment in which the animal is moving and the duty cycle of the tag. In general, PostgreSQL window functions can help.

In the next example, you will make use of window functions to convert the series of locations into steps (i.e. the straight-line segment connecting two successive locations), and compute geometric characteristics of each step: the time interval, the step length, and the distance moved during the step as the ratio of the previous two. It is important to note that while a step is the movement between two points, in many cases its attributes are associated with the starting or the ending point. In this book we use the ending point as reference. In some software, particularly the adehabitat package for R, the step is associated with the starting point. If needed, the queries and functions presented in this book can be modified to follow this convention.

```sql
SELECT 
  animals_id AS id, 
  acquisition_time, 
  LEAD(acquisition_time,-1) 
    OVER (
      PARTITION BY animals_id 
      ORDER BY acquisition_time) AS acquisition_time_1,
  (EXTRACT(epoch FROM acquisition_time) - 
  LEAD(EXTRACT(epoch FROM acquisition_time), -1) 
    OVER (
      PARTITION BY animals_id 
      ORDER BY acquisition_time))::integer AS deltat,
  (ST_Distance_Spheroid(
    geom, 
    LEAD(geom, -1) 
      OVER (
        PARTITION BY animals_id 
        ORDER BY acquisition_time), 
    'SPHEROID["WGS 84",6378137,298.257223563]'))::integer AS dist,
  (ST_Distance_Spheroid(
    geom, 
    LEAD(geom, -1) 
      OVER (
        PARTITION BY animals_id 
        ORDER BY acquisition_time), 
    'SPHEROID["WGS 84",6378137,298.257223563]')/
  ((EXTRACT(epoch FROM acquisition_time) - 
  LEAD(
    EXTRACT(epoch FROM acquisition_time), -1) 
    OVER (
      PARTITION BY animals_id 
      ORDER BY acquisition_time))+1)*60*60)::numeric(8,2) AS speed
FROM main.gps_data_animals 
WHERE gps_validity_code = 1
LIMIT 5;
```

As a demonstration of a possible approach to detecting 'impossible movements', here is an adapted function that implements the algorithm presented in Bjorneraas et al. (2010). In the first step, you compute the distance from each GPS position to the average of the previous and next 10 GPS positions, and extract records that have values bigger then a defined threshold (in this case, arbitrarily set to 10,000 meters):

```sql
SELECT gps_data_animals_id 
FROM 
  (SELECT 
    gps_data_animals_id, 
    ST_Distance_Spheroid(geom, 
      ST_setsrid(ST_makepoint(
        avg(ST_X(geom)) 
          OVER (
            PARTITION BY animals_id 
            ORDER BY acquisition_time rows 
              BETWEEN 10 PRECEDING and 10 FOLLOWING), 
        avg(ST_Y(geom)) 
          OVER (
            PARTITION BY animals_id 
            ORDER BY acquisition_time rows 
          BETWEEN 10 PRECEDING and 10 FOLLOWING)),
     4326),'SPHEROID["WGS 84",6378137,298.257223563]') AS dist_to_avg 
  FROM 
    main.gps_data_animals 
  WHERE 
    gps_validity_code = 1) a 
WHERE 
  dist_to_avg > 10000;

```
The result is the list of Ids, if any, of all the GPS positions that match the defined conditions (and thus can be considered outliers).

This code can be improved in many ways. For example, it is possible to consider the median instead of the average. It is also possible to take into consideration that the first and last 10 GPS positions have incomplete windows of 10+10 GPS positions. Moreover, this method works fine for GPS positions at regular time intervals, but in the case of a change in acquisition schedule might lead to unexpected results. In these cases, you should create a query with a temporal window instead of a fixed number of GPS positions.
In the second step, the angle and speed based on the previous and next GPS position is calculated (both the previous and next location are used to determine whether the location under consideration shows a spike in speed or turning angle), and then GPS positions below the defined thresholds (in this case, arbitrarily set as cosine of the relative angle < -0.99 and speed > 2500 meters per hour) are extracted:

```sql
SELECT 
  gps_data_animals_id 
FROM 
  (SELECT gps_data_animals_id, 
  ST_Distance_Spheroid(
    geom, 
    LAG(geom, 1) OVER (PARTITION BY animals_id ORDER BY acquisition_time), 'SPHEROID["WGS 84",6378137,298.257223563]') /
    (EXTRACT(epoch FROM acquisition_time) - EXTRACT (epoch FROM (lag(acquisition_time, 1) OVER (PARTITION BY animals_id ORDER BY acquisition_time))))*3600 AS speed_FROM,
  ST_Distance_Spheroid(
    geom, 
    LEAD(geom, 1) OVER (PARTITION BY animals_id ORDER BY acquisition_time), 'SPHEROID["WGS 84",6378137,298.257223563]') /
    ( - EXTRACT(epoch FROM acquisition_time) + EXTRACT (epoch FROM (lead(acquisition_time, 1) OVER (PARTITION BY animals_id ORDER BY acquisition_time))))*3600 AS speed_to,
  cos(ST_Azimuth((
    LAG(geom, 1) OVER (PARTITION BY animals_id ORDER BY acquisition_time))::geography, 
    geom::geography) - 
  ST_Azimuth(
    geom::geography, 
    (LEAD(geom, 1) OVER (PARTITION BY animals_id ORDER BY acquisition_time))::geography)) AS rel_angle
  FROM main.gps_data_animals 
  WHERE gps_validity_code = 1) a 
WHERE 
  rel_angle < -0.99 AND 
  speed_from > 2500 AND 
  speed_to > 2500;
```
The result returns the list of IDs of all the GPS positions that match the defined conditions. The same record detected in the previous query is returned. These examples can be used as templates to create other filtering procedures based on the temporal sequence of GPS positions and the users' defined movement constraints.
It is important to remember that this kind of method is based on the analysis of the sequence of GPS positions, and therefore results might change when new GPS positions are uploaded. Moreover, it is not possible to run them in real-time because the calculation requires a subsequent GPS position. The result is that they have to be run in a specific procedure unlinked with the (near) real-time import procedure.
Now you run this process on your data sets to mark the detected outliers (validity code '12'):

```sql
UPDATE 
  main.gps_data_animals 
SET 
  gps_validity_code = 12 
WHERE 
  gps_data_animals_id in
    (SELECT gps_data_animals_id 
    FROM 
      (SELECT 
        gps_data_animals_id, 
        ST_Distance_Spheroid(geom, lag(geom, 1) OVER (PARTITION BY animals_id ORDER BY acquisition_time), 'SPHEROID["WGS 84",6378137,298.257223563]') /
        (EXTRACT(epoch FROM acquisition_time) - EXTRACT (epoch FROM (lag(acquisition_time, 1) OVER (PARTITION BY animals_id ORDER BY acquisition_time))))*3600 AS speed_from,
        ST_Distance_Spheroid(geom, lead(geom, 1) OVER (PARTITION BY animals_id order by acquisition_time), 'SPHEROID["WGS 84",6378137,298.257223563]') /
        ( - EXTRACT(epoch FROM acquisition_time) + EXTRACT (epoch FROM (lead(acquisition_time, 1) OVER (PARTITION BY animals_id ORDER BY acquisition_time))))*3600 AS speed_to,
        cos(ST_Azimuth((lag(geom, 1) OVER (PARTITION BY animals_id ORDER BY acquisition_time))::geography, geom::geography) - ST_Azimuth(geom::geography, (lead(geom, 1) OVER (PARTITION BY animals_id ORDER BY acquisition_time))::geography)) AS rel_angle
      FROM main.gps_data_animals 
      WHERE gps_validity_code = 1) a 
    WHERE 
      rel_angle < -0.99 AND 
      speed_from > 2500 AND 
      speed_to > 2500);
```

The case of records that would imply improbable movements is similar to the previous type of error, but in this case the assumption made in the animals' movement model cannot completely exclude that the GPS position is correct (e.g. same methods as before, but with reduced thresholds: cosine of the relative angle < -0.98 and speed > 300 meters per hour). These records should be kept as valid but marked with a specific validity code that can permit users to exclude them for analysis as appropriate.

```sql
UPDATE 
  main.gps_data_animals 
SET 
  gps_validity_code = 2 
WHERE 
  gps_data_animals_id IN 
    (SELECT gps_data_animals_id 
    FROM 
      (SELECT 
        gps_data_animals_id, 
        ST_Distance_Spheroid(geom, lag(geom, 1) OVER (PARTITION BY animals_id ORDER BY acquisition_time), 'SPHEROID["WGS 84",6378137,298.257223563]') /
        (EXTRACT(epoch FROM acquisition_time) - EXTRACT (epoch FROM (lag(acquisition_time, 1) OVER (PARTITION BY animals_id ORDER BY acquisition_time))))*3600 AS speed_FROM,
        ST_Distance_Spheroid(geom, lead(geom, 1) OVER (PARTITION BY animals_id order by acquisition_time), 'SPHEROID["WGS 84",6378137,298.257223563]') /
        ( - EXTRACT(epoch FROM acquisition_time) + EXTRACT (epoch FROM (lead(acquisition_time, 1) OVER (PARTITION BY animals_id ORDER BY acquisition_time))))*3600 AS speed_to,
        cos(ST_Azimuth((lag(geom, 1) OVER (PARTITION BY animals_id ORDER BY acquisition_time))::geography, geom::geography) - ST_Azimuth(geom::geography, (lead(geom, 1) OVER (PARTITION BY animals_id ORDER BY acquisition_time))::geography)) AS rel_angle
      FROM main.gps_data_animals 
      WHERE gps_validity_code = 1) a 
    WHERE 
      rel_angle < -0.98 AND 
      speed_from > 300 AND 
      speed_to > 300);
```
The marked GPS positions should then be inspected visually to decide if they are valid with a direct expert evaluation.

